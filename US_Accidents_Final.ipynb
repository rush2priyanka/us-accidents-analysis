{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing libraries into this notebookâ€™s environment\n",
    "\n",
    "%pip install matplotlib folium seaborn scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective for the Random Forest Model**\n",
    "\n",
    "##### I am building a Random Forest model to predict the severity of accidents, so traffic authorities and emergency services can allocate resources more effectively and potentially reduce response times.\n",
    "##### These actionable insights that can help in planning, resource allocation, and public safety measures. Helping Local traffic authorities, emergency responders, or city planners who need to anticipate high-severity accidents and prepare accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Cleaning & Preprocessing: Cleaned the dataset (final_cleaned_data.csv) by handling missing values, capping outliers, and correcting negatives.\n",
    "##### EDA & Initial Feature Selection: Analyzed statistics, plots, and correlations to identify key features for predicting severity.\n",
    "##### Deeper Feature Importance Analysis: Plan to apply permutation importance/SHAP values to confirm which features most influence accident severity.\n",
    "##### Baseline Random Forest Model Training & Evaluation: Built an initial Random Forest model with a stratified split and evaluated it using classification metrics.\n",
    "##### Hyperparameter Tuning & Cross-Validation: Will use GridSearchCV and K-fold cross-validation to optimize model parameters and robustness.\n",
    "##### Additional Feature Engineering: Consider incorporating derived features (e.g., accident duration, time-based interactions) to further enhance predictions.\n",
    "##### Integration & Deployment Considerations: Outline steps for future model deployment and monitoring to support operational use.\n",
    "##### Overall Objective: Predict accident severity to help traffic authorities and emergency responders allocate resources effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Essential Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as ny\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Load our CSV file into a DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_df = pd.read_csv('10_US_Accidents_2016_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspecting Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Rows and Columns\n",
    "print(accidents_df.shape)\n",
    "# Inspecting the dataset structure\n",
    "print(accidents_df.info())\n",
    "# Inspect the dataset summary statistics\n",
    "print(accidents_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checking Severity Levels in dataset to base the predictive model on at a later stage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique severity values in dataset:\")\n",
    "print(accidents_df['Severity'].unique())\n",
    "\n",
    "print(\"\\nSeverity value counts in dataset:\")\n",
    "print(accidents_df['Severity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removing Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows \n",
    "accidents_df = accidents_df.drop_duplicates()\n",
    "accidents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Changing Format for columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm converting the Start_Time and End_Time columns to datetime using a known exact format.\n",
    "accidents_df['Start_Time'] = pd.to_datetime(accidents_df['Start_Time'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "accidents_df['End_Time']   = pd.to_datetime(accidents_df['End_Time'],   format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Now, I want to see if any rows failed to parse and were set to NaT.\n",
    "invalid_rows = accidents_df[accidents_df['Start_Time'].isna() | accidents_df['End_Time'].isna()]\n",
    "num_invalid = len(invalid_rows)\n",
    "\n",
    "if num_invalid > 0:\n",
    "    print(f\"{num_invalid} rows had invalid date/time formats and were set to NaT.\")\n",
    "else:\n",
    "    print(\"All rows successfully converted to datetime without errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, I'll verify that both columns are now in datetime format (not object).\n",
    "\n",
    "print(accidents_df.info())               # Shows overall DataFrame info, including dtypes\n",
    "print(accidents_df['Start_Time'].dtype)  \n",
    "print(accidents_df['End_Time'].dtype)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checking & Attending for Negative values, if found**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for negative values in the Distance(mi) column,\n",
    "# which are likely data entry errors or invalid entries.\n",
    "negative_distance_rows = accidents_df[accidents_df['Distance(mi)'] < 0]\n",
    "\n",
    "if not negative_distance_rows.empty:\n",
    "    print(\"Inconsistent values found (negative distances):\")\n",
    "    print(negative_distance_rows[['Distance(mi)', 'Start_Time', 'End_Time']])\n",
    "    \n",
    "    # Correct Approach Options:\n",
    "    # 1) Drop these rows if they are deemed invalid:\n",
    "    # accidents_df.drop(negative_distance_rows.index, inplace=True)\n",
    "    \n",
    "    # 2) Convert negative distances to positive if it's known they should be positive:\n",
    "    # accidents_df.loc[accidents_df['Distance(mi)'] < 0, 'Distance(mi)'] = \\\n",
    "    #     accidents_df['Distance(mi)'].abs()\n",
    "    \n",
    "    # 3) Set them to NaN for later imputation:\n",
    "    # accidents_df.loc[accidents_df['Distance(mi)'] < 0, 'Distance(mi)'] = float('nan')\n",
    "else:\n",
    "    print(\"No negative distances found.\")\n",
    "\n",
    "negative_distance_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Standardize Text Fields and Clean Categorical Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing text fields to ensure consistency in analysis:\n",
    "# 1) remove leading/trailing whitespace\n",
    "# 2) converted all text to lowercase\n",
    "text_columns = ['Street', 'City', 'County', 'State', 'Weather_Condition']\n",
    "\n",
    "for col in text_columns:\n",
    "    # Converting any non-string or missing values to string, then stripping and lowering.\n",
    "    accidents_df[col] = accidents_df[col].astype(str).str.strip().str.lower()\n",
    "\n",
    "accidents_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Identify & Inspect Categorical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining which columns I consider 'categorical' for further review.\n",
    "categorical_cols = [\n",
    "    'Street', 'City', 'County', 'State', 'Weather_Condition',\n",
    "    'Wind_Direction', 'Sunrise_Sunset', 'Civil_Twilight',\n",
    "    'Nautical_Twilight', 'Astronomical_Twilight'\n",
    "]\n",
    "\n",
    "# Will look at the first few unique values in each categorical column\n",
    "# to see what kind of data is available\n",
    "for col in categorical_cols:\n",
    "    unique_vals = accidents_df[col].unique()[:10]  # Show just the first 10 unique entries\n",
    "    print(f\"Unique values in '{col}' (showing up to 10): {unique_vals}\")\n",
    "    print(\"------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dropping Columns not needed for Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping these columns because they will not contribute significantly to the analysis of accident severity (Eg: country only have USA data , ID not neeeded etc)\n",
    "# I will revisit and potentially to drop more columns during the EDA phase as I gain further insights.\n",
    "columns_to_drop = ['ID', 'Airport_Code','Country', 'Description'] \n",
    "accidents_df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "accidents_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Identifying for outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach: Will use the Interquartile Range (IQR) method. \n",
    "# Rows that fall outside [Q1 - 1.5IQR, Q3 + 1.5IQR] for any numeric column are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing each numeric column to inspect for outliers\n",
    "\n",
    "numeric_cols = accidents_df.select_dtypes(include=[ny.number]).columns\n",
    "print(numeric_cols)\n",
    "for col in numeric_cols:\n",
    "    accidents_df.boxplot(column=col)\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Capping the outliers** : Created new DataFrame 'accidents_df_capped' where I exclude the Severity column from outlier capping so that I donâ€™t overwrite its values and lose other severity classes. Instead, all other numeric columns have been capped using the 1.5Ã—IQR method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame named 'accidents_df_capped'\n",
    "accidents_df_capped = accidents_df.copy()\n",
    "\n",
    "# Identify numeric columns but exclude 'Severity' to preserve its distribution\n",
    "numeric_cols = accidents_df_capped.select_dtypes(include=[ny.number]).columns\n",
    "numeric_cols = [col for col in numeric_cols if col != 'Severity']\n",
    "\n",
    "# Apply outlier capping using the 1.5 * IQR rule for each numeric column\n",
    "for col in numeric_cols:\n",
    "    Q1 = accidents_df_capped[col].quantile(0.25)\n",
    "    Q3 = accidents_df_capped[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    accidents_df_capped[col] = ny.where(accidents_df_capped[col] < lower_bound, lower_bound, accidents_df_capped[col])\n",
    "    accidents_df_capped[col] = ny.where(accidents_df_capped[col] > upper_bound, upper_bound, accidents_df_capped[col])\n",
    "\n",
    "print(\"Data shape after capping outliers (excluding 'Severity'):\", accidents_df_capped.shape)\n",
    "print(\"Unique severity values in 'accidents_df_capped':\", accidents_df_capped['Severity'].unique())\n",
    "print(\"\\nSeverity value counts in 'accidents_df_capped':\")\n",
    "print(accidents_df_capped['Severity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checking for Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm checking for missing values across all columns in accidents_df_capped.\n",
    "missing_counts = accidents_df_capped.isnull().sum()\n",
    "\n",
    "# Now will print only the columns that actually have missing values, along with their counts.\n",
    "print(\"Columns with missing values and their counts:\")\n",
    "for col, count in missing_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"{col}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Treating Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame for EDA after handling missing values\n",
    "cleaned_df = accidents_df_capped.copy()\n",
    "\n",
    "# Drop rows missing 'Start_Time' to ensure proper time-based forecasting\n",
    "cleaned_df.dropna(subset=['Start_Time'], inplace=True)\n",
    "\n",
    "# Dropped rows missing 'End_Time' to enable calculating accident duration if needed\n",
    "cleaned_df.dropna(subset=['End_Time'], inplace=True)\n",
    "\n",
    "# 'End_Lat' and 'End_Lng' have many missing values, and we already have 'Start_Lat'/'Start_Lng' for location, so I dropped them\n",
    "cols_to_drop = ['End_Lat', 'End_Lng']\n",
    "for col in cols_to_drop:\n",
    "    if col in cleaned_df.columns:\n",
    "        cleaned_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# For 'Precipitation(in)', assuming missing values mean no precipitation; filled them with 0\n",
    "if 'Precipitation(in)' in cleaned_df.columns:\n",
    "    cleaned_df['Precipitation(in)'].fillna(0, inplace=True)\n",
    "\n",
    "# Impute numeric weather columns with their median\n",
    "numeric_cols_median = [\n",
    "    'Temperature(F)',\n",
    "    'Wind_Chill(F)',\n",
    "    'Humidity(%)',\n",
    "    'Pressure(in)',\n",
    "    'Visibility(mi)',\n",
    "    'Wind_Speed(mph)'\n",
    "]\n",
    "for col in numeric_cols_median:\n",
    "    if col in cleaned_df.columns:\n",
    "        median_val = cleaned_df[col].median()\n",
    "        cleaned_df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# Impute categorical columns with their mode (most frequent value)\n",
    "categorical_cols_mode = [\n",
    "    'Zipcode',\n",
    "    'Timezone',\n",
    "    'Weather_Timestamp',\n",
    "    'Wind_Direction',\n",
    "    'Sunrise_Sunset',\n",
    "    'Civil_Twilight',\n",
    "    'Nautical_Twilight',\n",
    "    'Astronomical_Twilight'\n",
    "]\n",
    "for col in categorical_cols_mode:\n",
    "    if col in cleaned_df.columns:\n",
    "        mode_val = cleaned_df[col].mode(dropna=True)\n",
    "        if len(mode_val) > 0:\n",
    "            cleaned_df[col].fillna(mode_val[0], inplace=True)\n",
    "\n",
    "print(\"Shape of cleaned_df after handling missing values:\", cleaned_df.shape)\n",
    "print(\"\\nRemaining missing values:\")\n",
    "print(cleaned_df.isnull().sum())\n",
    "\n",
    "print(\"\\nSeverity value counts in cleaned_df:\")\n",
    "print(cleaned_df['Severity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EDA** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Summary Statistics, Distribution of Numerical Columns, Distribution of Categorical Columns, Correlation Analysis, Identify Relevant Features for Severity Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Basic Summary Statistics**: Quick review of min, max, mean, median, and standard deviation of numeric features to understand overall data spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Basic Summary Statistics\n",
    "# I'm printing basic statistics for numeric columns in our cleaned dataset.\n",
    "# This helps us understand the data spread (min, max, mean, median, etc.) for each numeric feature.\n",
    "\n",
    "print(\"Basic Summary Statistics for Numeric Columns:\")\n",
    "print(cleaned_df.describe())\n",
    "\n",
    "# Optionally, to view overall DataFrame info (including data types and non-null counts), uncomment the following:\n",
    "# print(\"\\nDataFrame Information:\")\n",
    "# print(cleaned_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Distribution of Numerical Columns** Visualizing each numeric feature in two ways (histograms and boxplots) to understand how values are spread out and spot any potential remaining anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Distribution of Numerical Columns\n",
    "# Plotting histograms and boxplots to see the spread of each numeric feature.\n",
    "\n",
    "num_cols = cleaned_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(data=cleaned_df, x=col, kde=True)\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(data=cleaned_df, x=col)\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Correlation Analysis** Generate a correlation matrix to identify which numeric features are most related to each other and to severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Correlation Analysis for Numeric Columns\n",
    "# A heatmap lets me see how strongly numeric features (including 'Severity') correlate.\n",
    "\n",
    "# Focusing only on numeric columns for correlation\n",
    "numeric_df = cleaned_df.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = numeric_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix for Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "# Print correlation of each numeric feature with 'Severity' if it's in the numeric columns\n",
    "if 'Severity' in numeric_df.columns:\n",
    "    severity_corr = corr_matrix['Severity'].sort_values(ascending=False)\n",
    "    print(\"Correlation of features with 'Severity':\")\n",
    "    print(severity_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Categorical Columns via Textual Summaries** how many unique categories each column has and prints the top five most frequent categories for a quick overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 : Categorical Columns via Textual Summaries\n",
    "cat_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(\"Textual Summary of Categorical Columns:\\n\")\n",
    "for col in cat_cols:\n",
    "    unique_vals = cleaned_df[col].nunique()\n",
    "    print(f\"{col}: {unique_vals} unique values\")\n",
    "    print(\"Top 5 Most Frequent Categories:\")\n",
    "    print(cleaned_df[col].value_counts().head(5))\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plot Only the Top N Categories** Instead of plotting all categories, it focuses on the top 10 most common categories in each column. This helps me visualize dominant categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 : Plot Only the Top N Categories\n",
    "\n",
    "cat_cols = cleaned_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    # Get the top 10 categories\n",
    "    top_10_categories = cleaned_df[col].value_counts().head(10).index\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.countplot(data=cleaned_df, x=col, order=top_10_categories)\n",
    "    plt.title(f'Top 10 Categories for {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Zipcode Extraction**: The code creates a new column called zipcode_5 by taking the first five characters of the original Zipcode field. This allows to retain a simplified version of the zipcode for later hotspot or geographic analysis.\n",
    "#### **Encode Before Plotting**: The code then label-encodes all other categorical columns. By removing zipcode_5 from the list of columns to be encoded, I ensured that the new column remains in its original string form for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Zipcode Handling and Label Encoding\n",
    "# I am extracting the first 5 digits from Zipcode for future analysis, \n",
    "# while label-encoding the other categorical columns.\n",
    "\n",
    "# Creating a new column 'zipcode_5' that stores the first 5 characters of Zipcode\n",
    "cleaned_df['zipcode_5'] = cleaned_df['Zipcode'].astype(str).str[:5]\n",
    "\n",
    "# Identify all categorical columns (object type)\n",
    "cat_cols = cleaned_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove 'zipcode_5' from cat_cols so it won't be label-encoded\n",
    "if 'zipcode_5' in cat_cols:\n",
    "    cat_cols.remove('zipcode_5')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label-encode each remaining categorical column\n",
    "for col in cat_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    cleaned_df[col] = encoder.fit_transform(cleaned_df[col])\n",
    "\n",
    "print(\"Categorical columns have been label-encoded.\")\n",
    "print(\"A new column 'zipcode_5' has been created to preserve the first 5 digits for future analysis.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Correlation Analysis after encoding categorical columns using only numeric features (including the previously categorical columns now encoded into numbers) and visualizing it as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running correlation analysis on updated DataFrame after encoding categorical columns\n",
    "numeric_df_updated = cleaned_df.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "plt.figure(figsize=(15, 13))\n",
    "corr_matrix_updated = numeric_df_updated.corr()\n",
    "sns.heatmap(corr_matrix_updated, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix After Encoding\")\n",
    "plt.show()\n",
    "\n",
    "if 'Severity' in numeric_df_updated.columns:\n",
    "    severity_corr_updated = corr_matrix_updated['Severity'].sort_values(ascending=False)\n",
    "    print(\"Correlation with 'Severity' after encoding:\")\n",
    "    print(severity_corr_updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Identifying relevant features for severity prediction**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping columns that are most likely useful based on domain knowledge (time, location, weather).\n",
    "# Dropping columns with near-zero or redundant impact (like 'Wind_Chill(F)' as we already have 'Temperature(F)').\n",
    "\n",
    "columns_to_keep = [\n",
    "    'Severity',\n",
    "    'Start_Time',        # needed for time-based features\n",
    "    'Distance(mi)',\n",
    "    'Temperature(F)',\n",
    "    'Pressure(in)',\n",
    "    'Humidity(%)',\n",
    "    'Visibility(mi)',\n",
    "    'Wind_Speed(mph)',\n",
    "    'Weather_Condition', # already label-encoded\n",
    "    'Sunrise_Sunset',    # label-encoded\n",
    "    'Civil_Twilight',    # label-encoded\n",
    "    'Nautical_Twilight', # label-encoded\n",
    "    'Astronomical_Twilight', # label-encoded\n",
    "    'City',              # label-encoded, might help for location-based patterns\n",
    "    'Zipcode',           # label-encoded, optional if not too high-cardinality\n",
    "    'Street',            # label-encoded, optional if not too high-cardinality\n",
    "    'State'              # label-encoded\n",
    "]\n",
    "\n",
    "cleaned_df = cleaned_df[columns_to_keep].copy()\n",
    "print(\"Remaining columns after selecting relevant features:\", cleaned_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Feature Engineering & Final Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns from 'Start_Time' and dropping it afterward (if I do not need them directly)\n",
    "\n",
    "# Convert Start_Time to datetime \n",
    "cleaned_df['Start_Time'] = pd.to_datetime(cleaned_df['Start_Time'], errors='coerce')\n",
    "\n",
    "# Extracting time-based features\n",
    "cleaned_df['Hour'] = cleaned_df['Start_Time'].dt.hour\n",
    "cleaned_df['DayOfWeek'] = cleaned_df['Start_Time'].dt.dayofweek\n",
    "cleaned_df['Month'] = cleaned_df['Start_Time'].dt.month\n",
    "\n",
    "# Drop Start_Time as it's no longer needed\n",
    "cleaned_df.drop(['Start_Time'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Shape after feature engineering & final cleaning:\", cleaned_df.shape)\n",
    "print(\"Current columns:\", cleaned_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proceeding to modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Forest to predict accident severity**: Evaluated performance using classification report and a confusion matrix to see how well the model distinguishes different severity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification Report shows** : The results show that it does an excellent job on the most frequent class (severity 2), but struggles with the less frequent severities (especially severity 4). \n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "This is a common challenge in imbalanced datasets, where the largest class tends to dominate. The classification report shows that the model performs very well on severity 2 accidentsâ€”with high precision (0.89) and recall (0.96)â€”but it struggles with the less frequent classes. For severity 1, the model is correct 75% of the time when predicted, but only captures 22% of actual cases. Similarly, severity 3 is moderately predicted (precision 0.81, recall 0.63), while severity 4 predictions are weak (precision 0.58, recall 0.26). Overall, the model achieves an accuracy of 88% and a weighted F1 score of 0.86, largely driven by the strong performance on severity 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the data, training a Random Forest, and evaluating performance.\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = cleaned_df.drop('Severity', axis=1)\n",
    "y = cleaned_df['Severity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)  # stratify to keep class balance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Memory error for above code so will proceed with samples of data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a new CSV with Cleaned DataFrame** : Will run Time Series model in a seperate file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the cleaned DataFrame to CSV\n",
    "cleaned_df.to_csv(\"final_cleaned_data.csv\", index=False)\n",
    "print(\"CSV file 'final_cleaned_data.csv' has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reducing the dataset size for a quick test & Feature Importance Visualization**: I am sampling a smaller subset (e.g., 20,000 rows) to ensure faster runtime and lower memory usage. Also, will plot which features are most influential in predicting accident severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce sample size to 20,000 rows\n",
    "sample_size = 20000\n",
    "\n",
    "# Sample a subset of X and align y accordingly\n",
    "X_small = X.sample(n=sample_size, random_state=42)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "# Split the smaller subset into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small, test_size=0.2, random_state=42, stratify=y_small\n",
    ")\n",
    "\n",
    "print(f\"Sampled dataset size: {X_small.shape}, train size: {X_train.shape}, test size: {X_test.shape}\")\n",
    "\n",
    "# Train a quick Random Forest model on the sampled data\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Feature Importance Visualization\n",
    "# I will now plot which features are most influential in predicting accident severity.\n",
    "###############################################################################\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=feat_imp_df, x='Importance', y='Feature')\n",
    "plt.title(\"Feature Importances (Sampled Dataset)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Brief Explanation of the Results**: I trained on 16,000 rows and tested on 4,000, which is a subset of the original dataset. This reduced memory usage and provided a quick check of the modelâ€™s feasibility.\n",
    "\n",
    "##### **Classification Report**: Precision, Recall, F1 are shown for each severity class (1, 2, 3, 4).The model predicts severity 2 very wellâ€”its precision and recall are very high, meaning it rarely makes mistakes and catches almost all severity 2 accidents. \n",
    "##### In contrast, it completely misses severity 1 and 4 (both precision and recall are 0) and only partly identifies severity 3. \n",
    "##### Overall accuracy is 83%, but that mainly reflects the performance on the dominant severity 2 class.\n",
    "\n",
    "##### **Feature Importances**: Street and Zipcode appear as the most influential features in this sample, followed by Distance(mi), City, Pressure(in), and Temperature(F). Time-based columns like Month and DayOfWeek have moderate importance, while Visibility(mi) seems less impactful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hyperparameter Tuning with GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code tests different Random Forest parameters (n_estimators, max_depth, and min_samples_split) on a smaller data sample to quickly find the best setup.\n",
    "#### 20,000 row sample to quickly identify the best hyperparameters. It then evaluates the best model on the test set using the F1 macro score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller sample for faster tuning\n",
    "sample_size = 20000\n",
    "X_small = X.sample(n=sample_size, random_state=42)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "# Split the sampled data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_small, y_small, test_size=0.2, random_state=42, stratify=y_small\n",
    ")\n",
    "\n",
    "# Define a grid of hyperparameters to test\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # number of trees\n",
    "    'max_depth': [None, 10, 20],  # maximum depth of trees\n",
    "    'min_samples_split': [2, 5]   # minimum samples to split a node\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV with 3-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf,          # The base Random Forest model we want to tune\n",
    "                           param_grid=param_grid, # The hyperparameter combinations to test\n",
    "                           cv=3,                # Use 3-fold cross-validation for each combination\n",
    "                           scoring='f1_macro',  # Evaluate each model with the macro F1 score\n",
    "                             n_jobs=-1,         # Use all available CPU cores to speed up the process\n",
    "                             verbose=1          # Print messages about the progress of the search\n",
    "                             )\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best F1 Macro Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(\"Classification Report on Test Data:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Result of Hyperparameter Tuning with GridSearchCV** shows that the best parameters and F1 macro score indicate how well the model balances precision and recall across all severity classes in the sampled dataset. \n",
    "#### The classification report on the test data shows that the model performs strongly on severity 2 but still struggles with minority classes (severity 1 and 4). This is typical in imbalanced datasets, where the model focuses on the dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cross-Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Cross-Validation?** It splits the data into multiple folds, training on several subsets and testing on the remaining fold each time. This provides a more stable measure of model performance compared to a single train/test split.\n",
    "#### I have used **Fewer Rows** (10k): Less data in memory, so the model training for each fold is lighter. **3-Fold Cross-Validation**: Reduces how many parallel models need to fit. **n_jobs=1**: Runs on a single core, avoiding large spikes in memory from parallel processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower sample size to 10,000 rows\n",
    "X_cv = X.sample(n=10000, random_state=42)\n",
    "y_cv = y.loc[X_cv.index]\n",
    "\n",
    "# Reinitialize the best model from the grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Use 3-fold CV and n_jobs=1 to reduce memory demands\n",
    "cv_scores = cross_val_score(\n",
    "    best_rf, X_cv, y_cv,\n",
    "    cv=3, scoring='f1_macro', n_jobs=1\n",
    ")\n",
    "\n",
    "print(\"Cross-validation F1 Macro scores:\", cv_scores)\n",
    "print(\"Mean F1 Macro score:\", ny.mean(cv_scores))\n",
    "print(\"Std of F1 Macro scores:\", ny.std(cv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of Cross-Validation Results** above show F1 Macro scores from the 3-fold cross-validation are around 0.346 each time, with a very small standard deviation (about 0.0008). \n",
    "#### This means the modelâ€™s performance on the sampled data is stable but relatively low overall, indicating it consistently struggles with the less frequent severity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Class Imbalance with Weighted Random Forest**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Class Weights?** In an imbalanced dataset, the model often ignores minority classes. By assigning class_weight=\"balanced\", each class influences the model more equally, potentially improving recall for severity 1 and 4.\n",
    "#### **Sampled Data**: I still use a subset (10,000 rows) to avoid memory issues and keep training time short.\n",
    "#### **Expected Outcome**: This approach may boost the modelâ€™s detection of less frequent severities, though it might slightly reduce overall accuracy on the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller sample for speed\n",
    "sample_size = 10000\n",
    "X_imb = X.sample(n=sample_size, random_state=42)\n",
    "y_imb = y.loc[X_imb.index]\n",
    "\n",
    "# Split the sampled data with stratification\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "# Assign class_weight=\"balanced\" so minority classes get higher weighting\n",
    "imb_rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "imb_rf_model.fit(X_train_imb, y_train_imb)\n",
    "y_pred_imb = imb_rf_model.predict(X_test_imb)\n",
    "\n",
    "print(\"Classification Report with Class Weights:\")\n",
    "print(classification_report(y_test_imb, y_pred_imb))\n",
    "print(\"\\nConfusion Matrix with Class Weights:\")\n",
    "print(confusion_matrix(y_test_imb, y_pred_imb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of Class Weight**: By giving more weight to underrepresented classes, the model slightly improves recall for minority severities (like 1 and 4), but it may still struggle due to the heavy class imbalance. Overall accuracy might drop a bit on the majority class (severity 2), but I'm now catching at least some of the minority classes instead of missing them entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Alternative Model (XGBoost) with Multi-Class Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'm using a smaller data sample again to compare XGBoost with our Random Forest approach. The goal is to see if XGBoost handles imbalanced classes better or yields higher performance.\n",
    "#### I encode severity labels (1,2,3,4) to 0..3 so XGBoost can handle them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a smaller sample for faster training\n",
    "sample_size = 10000\n",
    "X_xgb = X.sample(n=sample_size, random_state=42)\n",
    "y_xgb = y.loc[X_xgb.index]\n",
    "\n",
    "# Encode severity labels from {1,2,3,4} to {0,1,2,3}\n",
    "# so XGBoost recognizes each class as an integer in [0..num_class-1].\n",
    "y_encoder = LabelEncoder()\n",
    "y_xgb_encoded = y_encoder.fit_transform(y_xgb)  # e.g., 1->0, 2->1, etc.\n",
    "\n",
    "# Splitting the sampled data (with stratification)\n",
    "X_train_xgb, X_test_xgb, y_train_xgb_enc, y_test_xgb_enc = train_test_split(\n",
    "    X_xgb, y_xgb_encoded, test_size=0.2, random_state=42, \n",
    "    stratify=y_xgb_encoded\n",
    ")\n",
    "\n",
    "# Initialize an XGBoost classifier for multi-class (4 classes)\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softmax',  # multi-class classification\n",
    "    num_class=4,               # we have 4 severity classes\n",
    "    n_estimators=100, \n",
    "    max_depth=6, \n",
    "    random_state=42,\n",
    "    use_label_encoder=False,   # disable deprecation warning\n",
    "    eval_metric='mlogloss'     # default multi-class metric\n",
    ")\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb_enc)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb_enc = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Decode predictions back to original labels (1..4)\n",
    "y_pred_xgb = y_encoder.inverse_transform(y_pred_xgb_enc)\n",
    "\n",
    "# Decode test labels too, for a proper comparison in the report\n",
    "y_test_xgb = y_encoder.inverse_transform(y_test_xgb_enc)\n",
    "\n",
    "# Evaluate the XGBoost model\n",
    "print(\"Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test_xgb, y_pred_xgb))\n",
    "\n",
    "print(\"\\nConfusion Matrix (XGBoost):\")\n",
    "print(confusion_matrix(y_test_xgb, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results Explanation for XGBoost**: Model mostly predicts severity 2 and 4 well, but it completely misses severity 1 and only partly captures severity 3. Overall, while the model has an accuracy of 80%, its performance is poor for the less common classes, highlighting that the class imbalance is still a major issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary**\n",
    "\n",
    "### **Initial Model**: The baseline Random Forest performed well for the dominant class (severity 2) but completely missed some minority classes, highlighting a common imbalance issue.\n",
    "\n",
    "### **Feature Importance**: A quick Random Forest run showed which features drive the modelâ€™s predictions, confirming the importance of key weather and location variables.\n",
    "\n",
    "### **Model Refinement:**: Hyperparameter tuning and cross-validation helped improve overall performance, although class imbalance still limits detection of less frequent severities.\n",
    "\n",
    "### **Alternative Approach (XGBoost)**: XGBoost showed promise, with an overall accuracy of around 80%. However, while it improved predictions for severity 2 and 4, it still failed to detect severity 1 and only partially captured severity 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion & Next Steps**\n",
    "\n",
    "#### Given the current results and time constraints, I am concluding the modeling phase here.\n",
    "\n",
    "### **Limitations**:\n",
    "#### (a) Persistent class imbalance, where minority classes (severity 1 and 3) remain poorly predicted. (b) Limited performance gains from hyperparameter tuning due to inherent data imbalances.\n",
    "\n",
    "### **Future Improvements**:\n",
    "### (a) Experiment with advanced resampling techniques (e.g., SMOTE) to handle class imbalance.(b) Test additional ensemble methods or boosting algorithms (or even deep learning models) to further enhance performance. (c) Incorporate more external or derived features (e.g., accident duration, detailed weather conditions) for richer predictions.\n",
    "\n",
    "### This comprehensive approachâ€”from data cleaning and EDA through feature importance analysis and alternative modelingâ€”has provided valuable insights into predicting accident severity. Although my model performs well on the dominant class, future work will focus on improving detection for minority severities and exploring further model enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
